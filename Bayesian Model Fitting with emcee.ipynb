{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian model fitting with *emcee*\n",
    "### [Giuliano Antoniciello](http://www.dfa.unipd.it/index.php?id=1744)\n",
    "### Py@stro - April 24th, 2018\n",
    "\n",
    "The [** *emcee* **](http://dfm.io/emcee/current/) MCMC sampler is a powerful tool to perform complex and computationally daunting Bayesian inference.\n",
    "\n",
    "We will review the fundamentals of Bayesian probability theory and apply them to some model fitting examples in Astronomy. The needed Python libraries are:\n",
    "\n",
    "1. [*emcee*](http://dfm.io/emcee/current/) to run the MCMC simulations;\n",
    "2. [*corner*](https://corner.readthedocs.io/en/latest/) to show the results.\n",
    "\n",
    "To run this notebook you'll need also the following Python libraries:\n",
    "\n",
    "1. [*Numpy*](http://www.numpy.org/)\n",
    "2. [*Matplotlib*](https://matplotlib.org/)\n",
    "3. [*Seaborn*](https://seaborn.pydata.org/) (optional)\n",
    "\n",
    "**WARNING: this notebook is intended for a Python 2.7 kernel!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The linear model\n",
    "#### Let's generate a synthetic dataset\n",
    "First we draw some data points from our parametric model and add a Gaussian noise.\n",
    "Our aim is to determine the most probable values for the model parameters. In this simple example the target function is just a linear relation with two parameters:\n",
    "\n",
    "$f(x) = mx+q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the number of data points\n",
    "n_points = 25\n",
    "\n",
    "# Set the true values of model parameters\n",
    "m_true = 5.0\n",
    "q_true = -3.0\n",
    "\n",
    "# Set the pointwise uncertainty\n",
    "sigma = 5.0\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "x = np.sort(10*np.random.random(size=n_points))\n",
    "y_true = m_true*x + q_true\n",
    "\n",
    "# Add Gaussian noise and error bars\n",
    "y = y_true + np.random.normal(loc=0.,scale=sigma, size=n_points)\n",
    "yerr = sigma*np.ones(n_points)\n",
    "\n",
    "# Compute the reduced chi square and the initial log-likelihood\n",
    "num = (y_true-y)*(y_true-y)\n",
    "den = yerr*yerr\n",
    "chi_square = np.sum(np.true_divide(num, den))\n",
    "reduced_chi_square = chi_square/(n_points - 2)\n",
    "\n",
    "# log(error)\n",
    "log_err = np.sum(np.log(yerr))\n",
    "\n",
    "# log_likelihood value\n",
    "true_log_likelihood = -(0.5*n_points*np.log(2*np.pi)) - log_err - (0.5 * chi_square)\n",
    "print \"Reduced chi square: \" + str(reduced_chi_square)\n",
    "print \"Log-likelihood: \" + str(true_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seaborn-style plot\n",
    "# fancy plots :)\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks')\n",
    "sns.set_color_codes()\n",
    "\n",
    "# Plot the data points and the target function\n",
    "import matplotlib.pyplot as plt\n",
    "plt.errorbar(x,y,yerr,fmt='o',color='r')\n",
    "plt.plot(x,y_true, color='b', alpha=0.7)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model function\n",
    "Here we define a function that takes an $x$ value and return the corresponding $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function for our linear model (as suggested by prior information)\n",
    "def linear_model(x, m, q):\n",
    "    \"\"\"\n",
    "    Linear model for the target function\n",
    "\n",
    "    Input:\n",
    "    -----\n",
    "\n",
    "    - x, independet variable (some units)\n",
    "    - m, slope\n",
    "    - q, constant term\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    - f(x), the value of target function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plain and simple: a linear model\n",
    "    model = m*x + q\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (log) prior probability\n",
    "Notice that *emcee* always uses log-probabilities! \n",
    "Here the log-prior probability takes data and model parameters' values $\\theta$ and returns a log-probability:\n",
    "\n",
    "$lnprior = p(\\theta \\mid DX)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for uniform prior probabilities\n",
    "def lnprior(theta, x_vector, y_vector, yerr_vector, m_range, q_range):\n",
    "    \n",
    "    \"\"\"\n",
    "    Log-prior probability function\n",
    "\n",
    "    Input:\n",
    "    -----\n",
    "\n",
    "    - theta, the model parameters (m, q)\n",
    "    - x_vector, array of independet variable values (some units)\n",
    "    - y_vector, array of f(x) values\n",
    "    - yerr_vector, array of uncertainties for the f(x) value\n",
    "    - m_range, tuple with lower and upper limits for the prior interval of m\n",
    "    - q_range, tuple with lower and upper limits for the prior interval of q\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    - log(prior(theta)), logarithm of prior probability for the set of value parameters theta\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # Unpack the parameters\n",
    "    m, q = theta\n",
    "\n",
    "    # Uninformative prior for m\n",
    "    #--------------------------\n",
    "    # Extract the interval's limits\n",
    "    m_min, m_max = m_range\n",
    "    # Compute the interval's lenght\n",
    "    delta_m = m_max - m_min\n",
    "    # Condition for the uniform prior\n",
    "    m_cond = m_min < m < m_max\n",
    "    \n",
    "    # Uninformative prior for q\n",
    "    #--------------------------\n",
    "    # Extract the interval's limits\n",
    "    q_min, q_max = q_range\n",
    "    # Compute the interval's lenght\n",
    "    delta_q = q_max - q_min\n",
    "    # Condition for the uniform prior\n",
    "    q_cond = q_min < m < q_max\n",
    "\n",
    "    # Global condition for the priors\n",
    "    cond = m_cond and q_cond\n",
    "    \n",
    "    # Evaluate the condition\n",
    "    if cond:\n",
    "        return -np.log(delta_m * delta_q)\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (log) likelihood\n",
    "The likelihood function, from the point-of-view of a fixed dataset, is not a probability, since it's not normalized to unit. It a function of model parameters expressing \"how liklely\" is a given set of parameter values once we observe the data $D$ and have the prior information $X$. Indeed, the same term (likelihood) can be interpreted as a probability if we consider a fixed set of parameter values: in this case, likelihood is the sampling distribution from which data points are drawn. Since collecting data is typically a result of many complex operation, we communly assume a Gaussian likelihood for statistically independent measures:\n",
    "\n",
    "$likelihood = p(D \\mid \\theta X) = \\prod_{i=1}^{n_{points}} \\dfrac{1}{\\sqrt{2 \\pi} \\sigma_i} \\exp{-\\dfrac{(x_i - f(x_i))^2}{2 \\sigma_i^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for the natural logarithm of the likelihood\n",
    "def lnlike(theta, x_vector, y_vector, yerr_vector, m_range, q_range):\n",
    "    \n",
    "    \"\"\"\n",
    "    Log-prior probability function\n",
    "\n",
    "    Input:\n",
    "    -----\n",
    "\n",
    "    - theta, the model parameters (m, q)\n",
    "    - x_vector, array of independet variable values (some units)\n",
    "    - y_vector, array of f(x) values\n",
    "    - yerr_vector, array of uncertainties for the f(x) value\n",
    "    - m_range, tuple with lower and upper limits for the prior interval of m\n",
    "    - q_range, tuple with lower and upper limits for the prior interval of q\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    - log(likelihood(theta)), logarithm of the likelihood function for the set of value parameters theta\n",
    "    \"\"\"\n",
    "    \n",
    "    #---------------------------------------\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the parameter values\n",
    "    m, q = theta\n",
    "    \n",
    "    # Get the model values vector\n",
    "    model_vector = np.zeros(x_vector.shape[0])\n",
    "    for index, x in enumerate(x_vector):\n",
    "        model_vector[index] = linear_model(x, m, q)\n",
    "\n",
    "    # Compute the chi^2\n",
    "    num = (y_vector - model_vector) * (y_vector - model_vector)\n",
    "    den = yerr_vector * yerr_vector\n",
    "    chi_squared = np.sum(np.true_divide(num, den))\n",
    "\n",
    "    # Number of data points\n",
    "    n_points = x_vector.shape[0]\n",
    "\n",
    "    # Compute the sum of log(uncertainties)\n",
    "    log_err = np.sum(np.log(yerr_vector))\n",
    "\n",
    "    # Compute the log_likelihood value\n",
    "    log_likelihood = -(0.5*n_points*np.log(2*np.pi)) - log_err - (0.5 * chi_squared)\n",
    "\n",
    "    #---------------------------------------\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (log) posterior probability\n",
    "Log-posterior probability is simply\n",
    "\n",
    "$log(posterior) = log(prior) + log(likelihood)$\n",
    "\n",
    "since, from the Bayes theorem\n",
    "\n",
    "$posterior \\propto prior \\times likelihood$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, x_vector, y_vector, yerr_vector, m_range, q_range):\n",
    "    \n",
    "    \"\"\"\n",
    "    Log-posterior probability function\n",
    "\n",
    "    Input:\n",
    "    -----\n",
    "\n",
    "    - theta, the model parameters (m, q)\n",
    "    - x_vector, array of independet variable values (some units)\n",
    "    - y_vector, array of f(x) values\n",
    "    - yerr_vector, array of uncertainties for the f(x) value\n",
    "    - m_range, tuple with lower and upper limits for the prior interval of m\n",
    "    - q_range, tuple with lower and upper limits for the prior interval of q\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "\n",
    "    - log(posterior(theta)), logarithm of the likelihood function for the set of value parameters theta\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    # Compute prior probability for the current walker position\n",
    "    lp = lnprior(theta, x_vector, y_vector, yerr_vector, m_range, q_range)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x_vector, y_vector, yerr_vector, m_range, q_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the MCMC sampler\n",
    "##### Affine invariant sampler by [*Goodman & Weare (2010)*](https://msp.org/camcos/2010/5-1/p04.xhtml)\n",
    "This algorithm has been design to overcome the difficulties that standard MCMC algorithms (such as Metropolis-Hastings) encounter in treating highly non linear models and (possibly strongly) correlated parameters. See the reference for mathematical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the n. of walkers and n. of parameters (--> dimension of the parameter space)\n",
    "n_dim = 2\n",
    "n_walkers = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of iteration to be performed\n",
    "N_iter = 2000\n",
    "# Set the burn-in steps\n",
    "burn_in = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset as arguments of the MCMC simulation\n",
    "m_range = (0.,10.)\n",
    "q_range = (-10.,10.)\n",
    "args = (x, y, yerr, m_range, q_range)\n",
    "\n",
    "# New object from the emcee class\n",
    "# Instance for the MCMC simulation\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers=n_walkers, dim=n_dim, lnpostfn=lnprob, a=2.0, args=args, threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the seed for the Mersenne-Twister random number generatore\n",
    "np.random.seed(42)\n",
    "# Inizialize the walkers in a small hyper-sphere\n",
    "p0 = np.array([[m_true, q_true] + (10e-4)*np.random.normal(loc=0.0, scale=1.0, size=2) \\\n",
    "               for i in xrange(n_walkers)], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instance for the sample generator\n",
    "samples = sampler.sample(p0=p0, rstate0=52, iterations=N_iter, thin=1, storechain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the 3-D matrices for the walker positions at each step (chains) and the logposterior values (posteriors)\n",
    "# Each slice of these 3-D matrices corresponds to a single iteration step\n",
    "# with walkers on the rows and parameters on the columns\n",
    "chains = np.zeros((n_walkers, n_dim, N_iter))\n",
    "posteriors = np.zeros((n_walkers, 1, N_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run the MCMC simualtion!\n",
    "So far we've just instantiated the *emcee* object and the sample generator. Now we go through the computationally expensive part of our simulation: drawing and storing the samples from the posterior pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the posterior values from the sample generator\n",
    "for index, sample in enumerate(samples):\n",
    "    # Positions of walkers in the parameter space\n",
    "    positions_matrix = sample[0]\n",
    "    chains[:, :, index] = positions_matrix\n",
    "\n",
    "    # Log values of the posteriors\n",
    "    logposteriors_matrix = sample[1]\n",
    "    posteriors[:, 0, index] = logposteriors_matrix\n",
    "    \n",
    "# Sample matrices\n",
    "m_samples = chains[:,0,burn_in:].reshape(((N_iter-burn_in)*n_walkers,1))\n",
    "q_samples = chains[:,1,burn_in:].reshape(((N_iter-burn_in)*n_walkers,1))\n",
    "\n",
    "# Posteriors matrix\n",
    "post = np.column_stack((m_samples, q_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use [*corner.py*](https://corner.readthedocs.io/en/latest/) to plot the chains and the posterior pdfs\n",
    "Corner plots summarize the information about posterior pdfs and correlation between parameters in a simple triangular plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "labels = [r\"$m$\", r\"$q$\"]\n",
    "\n",
    "figure = corner.corner(post, labels=labels, \\\n",
    "quantiles=[0.16, 0.5, 0.84], \\\n",
    "show_titles=True, title_kwargs={\"fontsize\": 14}, color=\"k\", title_fmt=\".2f\", \\\n",
    "use_math_text = True, truths=[m_true, q_true])\n",
    "\n",
    "figure.savefig(\"corner_plot.pdf\", format=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore the chains in the \"physical\" space ($x$, $f(x)$)\n",
    "Here we show how the couples of ($m$, $q$) stored in the chains behave in the plot $x$ *vs* $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Plot the data points and the target function\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close()\n",
    "plt.errorbar(x,y,yerr,fmt='o',color='r', label='Data points')\n",
    "n_samples = m_samples.shape[0]\n",
    "x_line = np.linspace(0,10,10)\n",
    "for m, q in post[::20,:]:\n",
    "    y_true = m*x_line+q\n",
    "    plt.plot(x_line,y_true, color='b', alpha=0.01)\n",
    "plt.plot(x_line, m_true*x_line+q_true, color='k', label='Target function')\n",
    "plt.xlabel('$x$', fontsize=20)\n",
    "plt.ylabel('$f(x)$', fontsize=20)\n",
    "plt.legend(fontsize=20, loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
